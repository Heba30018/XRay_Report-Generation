# -*- coding: utf-8 -*-
"""Untitled38.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1MCC3Zu7w4GSrsE3IMnhfuzxNRIiOkjr-
"""

import pandas as pd
from torch.utils.data import Dataset, DataLoader
from transformers import (
    Seq2SeqTrainingArguments,
    Seq2SeqTrainer,
    default_data_collator,
    VisionEncoderDecoderModel,
    AutoFeatureExtractor,
    AutoTokenizer
)

# Define Dataset class
class LoadDataset(Dataset):
    def __init__(self, df, feature_extractor, tokenizer, max_length):
        self.images = df['imgs'].values
        self.captions = df['captions'].values
        self.feature_extractor = feature_extractor
        self.tokenizer = tokenizer
        self.max_length = max_length

    def __getitem__(self, idx):
        inputs = dict()
        image_path = str(self.images[idx])
        image = Image.open(image_path).convert("RGB")
        image = self.feature_extractor(images=image, return_tensors='pt')
        caption = self.captions[idx]
        labels = self.tokenizer(
            caption,
            max_length=self.max_length,
            truncation=True,
            padding='max_length',
            return_tensors='pt'
        )['input_ids'][0]
        inputs['pixel_values'] = image['pixel_values'].squeeze()
        inputs['labels'] = labels
        return inputs

    def __len__(self):
        return len(self.images)

# Load data
train_df = pd.read_csv('train.csv')
val_df = pd.read_csv('val.csv')

# Load feature_extractor, tokenizer, and model
feature_extractor = AutoFeatureExtractor.from_pretrained('feature_extractor')
tokenizer = AutoTokenizer.from_pretrained('tokenizer')
model = VisionEncoderDecoderModel.from_pretrained('model')

# Create datasets
max_length = 1024
train_ds = LoadDataset(train_df, feature_extractor, tokenizer, max_length)
val_ds = LoadDataset(val_df, feature_extractor, tokenizer, max_length)

# Define training arguments
training_args = Seq2SeqTrainingArguments(
    output_dir="image-caption-generator",
    evaluation_strategy="epoch",
    per_device_train_batch_size=4,
    per_device_eval_batch_size=4,
    learning_rate=5e-5,
    weight_decay=0.01,
    num_train_epochs=2,
    save_strategy='epoch',
    report_to='wandb',
    logging_dir="./logs",
    logging_steps=10,
)

# Define Trainer
trainer = Seq2SeqTrainer(
    model=model,
    tokenizer=feature_extractor,
    data_collator=default_data_collator,
    train_dataset=train_ds,
    eval_dataset=val_ds,
    args=training_args,
)

# Train model
trainer.train()