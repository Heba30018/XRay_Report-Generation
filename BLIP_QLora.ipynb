{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":951996,"sourceType":"datasetVersion","datasetId":516716}],"dockerImageVersionId":30665,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -q git+https://github.com/huggingface/peft.git transformers bitsandbytes datasets","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-04-12T16:15:12.346247Z","iopub.execute_input":"2024-04-12T16:15:12.346521Z","iopub.status.idle":"2024-04-12T16:15:45.737927Z","shell.execute_reply.started":"2024-04-12T16:15:12.346495Z","shell.execute_reply":"2024-04-12T16:15:45.736881Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"!pip install optimum","metadata":{"execution":{"iopub.status.busy":"2024-04-12T16:15:45.739880Z","iopub.execute_input":"2024-04-12T16:15:45.740172Z","iopub.status.idle":"2024-04-12T16:15:59.371554Z","shell.execute_reply.started":"2024-04-12T16:15:45.740144Z","shell.execute_reply":"2024-04-12T16:15:59.370434Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Collecting optimum\n  Downloading optimum-1.18.1-py3-none-any.whl.metadata (18 kB)\nCollecting coloredlogs (from optimum)\n  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from optimum) (1.12)\nRequirement already satisfied: transformers<4.40.0,>=4.26.0 in /opt/conda/lib/python3.10/site-packages (from transformers[sentencepiece]<4.40.0,>=4.26.0->optimum) (4.38.1)\nRequirement already satisfied: torch>=1.11 in /opt/conda/lib/python3.10/site-packages (from optimum) (2.1.2)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from optimum) (21.3)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from optimum) (1.26.4)\nRequirement already satisfied: huggingface-hub>=0.8.0 in /opt/conda/lib/python3.10/site-packages (from optimum) (0.20.3)\nRequirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (from optimum) (2.1.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.8.0->optimum) (3.13.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.8.0->optimum) (2024.2.0)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.8.0->optimum) (2.31.0)\nRequirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.8.0->optimum) (4.66.1)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.8.0->optimum) (6.0.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.8.0->optimum) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->optimum) (3.1.1)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.11->optimum) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.11->optimum) (3.1.2)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers<4.40.0,>=4.26.0->transformers[sentencepiece]<4.40.0,>=4.26.0->optimum) (2023.12.25)\nRequirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.10/site-packages (from transformers<4.40.0,>=4.26.0->transformers[sentencepiece]<4.40.0,>=4.26.0->optimum) (0.15.2)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers<4.40.0,>=4.26.0->transformers[sentencepiece]<4.40.0,>=4.26.0->optimum) (0.4.2)\nRequirement already satisfied: sentencepiece!=0.1.92,>=0.1.91 in /opt/conda/lib/python3.10/site-packages (from transformers[sentencepiece]<4.40.0,>=4.26.0->optimum) (0.2.0)\nRequirement already satisfied: protobuf in /opt/conda/lib/python3.10/site-packages (from transformers[sentencepiece]<4.40.0,>=4.26.0->optimum) (3.20.3)\nCollecting humanfriendly>=9.1 (from coloredlogs->optimum)\n  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\nRequirement already satisfied: pyarrow>=5.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets->optimum) (11.0.0)\nRequirement already satisfied: dill in /opt/conda/lib/python3.10/site-packages (from datasets->optimum) (0.3.8)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets->optimum) (2.1.4)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets->optimum) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets->optimum) (0.70.16)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets->optimum) (3.9.1)\nRequirement already satisfied: responses<0.19 in /opt/conda/lib/python3.10/site-packages (from datasets->optimum) (0.18.0)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->optimum) (1.3.0)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->optimum) (23.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->optimum) (6.0.4)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->optimum) (1.9.3)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->optimum) (1.4.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->optimum) (1.3.1)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->optimum) (4.0.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.8.0->optimum) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.8.0->optimum) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.8.0->optimum) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.8.0->optimum) (2024.2.2)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.11->optimum) (2.1.3)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->optimum) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->optimum) (2023.3.post1)\nRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->optimum) (2023.4)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets->optimum) (1.16.0)\nDownloading optimum-1.18.1-py3-none-any.whl (410 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.0/410.0 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: humanfriendly, coloredlogs, optimum\nSuccessfully installed coloredlogs-15.0.1 humanfriendly-10.0 optimum-1.18.1\n","output_type":"stream"}]},{"cell_type":"code","source":"import pandas as pd\ndf2 = pd.read_csv('/kaggle/input/chest-xrays-indiana-university/indiana_projections.csv')\ndf1 = pd.read_csv('/kaggle/input/chest-xrays-indiana-university/indiana_reports.csv')\nimages_captions_df = pd.DataFrame({'imgs': [],\n                                    'captions': []})\nfor i in range(len(df2)):\n    uid = df2.iloc[i]['uid']\n    image = df2.iloc[i]['filename']\n    index = df1.loc[df1['uid'] ==uid]\n    \n    if not index.empty:    \n        index = index.index[0]\n        caption = df1.iloc[index]['findings']\n        if type(caption) == float:\n         \n            continue \n        images_captions_df = pd.concat([images_captions_df, pd.DataFrame([{'imgs': image, 'captions': caption}])], ignore_index=True)\np = '/kaggle/input/chest-xrays-indiana-university/images/images_normalized/'\nimages_captions_df['imgs'] = p+ images_captions_df['imgs']\nimages_captions_df.head()","metadata":{"execution":{"iopub.status.busy":"2024-04-12T16:15:59.376857Z","iopub.execute_input":"2024-04-12T16:15:59.377121Z","iopub.status.idle":"2024-04-12T16:16:07.684575Z","shell.execute_reply.started":"2024-04-12T16:15:59.377095Z","shell.execute_reply":"2024-04-12T16:16:07.683590Z"},"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"                                                imgs  \\\n0  /kaggle/input/chest-xrays-indiana-university/i...   \n1  /kaggle/input/chest-xrays-indiana-university/i...   \n2  /kaggle/input/chest-xrays-indiana-university/i...   \n3  /kaggle/input/chest-xrays-indiana-university/i...   \n4  /kaggle/input/chest-xrays-indiana-university/i...   \n\n                                            captions  \n0  The cardiac silhouette and mediastinum size ar...  \n1  The cardiac silhouette and mediastinum size ar...  \n2  Borderline cardiomegaly. Midline sternotomy XX...  \n3  Borderline cardiomegaly. Midline sternotomy XX...  \n4  There are diffuse bilateral interstitial and a...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>imgs</th>\n      <th>captions</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>/kaggle/input/chest-xrays-indiana-university/i...</td>\n      <td>The cardiac silhouette and mediastinum size ar...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>/kaggle/input/chest-xrays-indiana-university/i...</td>\n      <td>The cardiac silhouette and mediastinum size ar...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>/kaggle/input/chest-xrays-indiana-university/i...</td>\n      <td>Borderline cardiomegaly. Midline sternotomy XX...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>/kaggle/input/chest-xrays-indiana-university/i...</td>\n      <td>Borderline cardiomegaly. Midline sternotomy XX...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>/kaggle/input/chest-xrays-indiana-university/i...</td>\n      <td>There are diffuse bilateral interstitial and a...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"from torch.utils.data import Dataset, DataLoader\nfrom PIL import Image\n\nclass ImageCaptioningDataset(Dataset):\n    def __init__(self, df,processor):\n        self.images = df['imgs'].values\n        self.captions = df['captions'].values\n        self.processor = processor\n\n    def __len__(self):\n        return len(self.images)\n\n    def __getitem__(self, idx):\n        image_path = str(self.images[idx])\n        image = Image.open(image_path).convert(\"RGB\")\n        caption = self.captions[idx]\n       \n        encoding = self.processor(images=image, padding=\"max_length\", return_tensors=\"pt\")\n        # remove batch dimension\n        encoding = {k: v.squeeze() for k, v in encoding.items()}\n        encoding[\"text\"] = caption\n        return encoding\n\ndef collate_fn(batch):\n    # pad the input_ids and attention_mask\n    processed_batch = {}\n    for key in batch[0].keys():\n        if key != \"text\":\n            processed_batch[key] = torch.stack([example[key] for example in batch])\n        else:\n            text_inputs = processor.tokenizer(\n                [example[\"text\"] for example in batch], padding=True, return_tensors=\"pt\"\n            )\n            processed_batch[\"input_ids\"] = text_inputs[\"input_ids\"]\n            processed_batch[\"attention_mask\"] = text_inputs[\"attention_mask\"]\n    return processed_batch\n","metadata":{"execution":{"iopub.status.busy":"2024-04-12T16:16:07.685788Z","iopub.execute_input":"2024-04-12T16:16:07.686068Z","iopub.status.idle":"2024-04-12T16:16:11.223886Z","shell.execute_reply.started":"2024-04-12T16:16:07.686043Z","shell.execute_reply":"2024-04-12T16:16:11.223103Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"from transformers import Blip2ForConditionalGeneration, AutoProcessor\nfrom peft import PeftModel, PeftConfig\n\npeft_model_id = \"shorok31/BLIP_LORA_2\"\nconfig = PeftConfig.from_pretrained(peft_model_id)\n\nmodel = Blip2ForConditionalGeneration.from_pretrained(config.base_model_name_or_path, load_in_8bit=True, device_map=\"auto\")\nmodel = PeftModel.from_pretrained(model, peft_model_id)\n\nprocessor = AutoProcessor.from_pretrained(\"shorok31/BLIP_LORA_2\")","metadata":{"execution":{"iopub.status.busy":"2024-04-13T01:37:11.657179Z","iopub.execute_input":"2024-04-13T01:37:11.658000Z","iopub.status.idle":"2024-04-13T01:37:26.101313Z","shell.execute_reply.started":"2024-04-13T01:37:11.657967Z","shell.execute_reply":"2024-04-13T01:37:26.100323Z"},"trusted":true},"execution_count":41,"outputs":[{"output_type":"display_data","data":{"text/plain":"adapter_config.json:   0%|          | 0.00/780 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ce5eb260cb2d411ca17681c5d4ab46a2"}},"metadata":{}},{"name":"stderr","text":"The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"275d1fca226c4d6ab1d9d202e1b2bfc8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"adapter_model.safetensors:   0%|          | 0.00/21.0M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"884ecf5fb51c4bd389c88c9770e42196"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/432 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"23a2263b0b884546b2b4a392a34c27ae"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/813 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"24bc929c953444ee82014965457c018e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/798k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"71a59a804d7b43cf932096a71b2bca0a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3f166dabf07948dd9ff005d4a1744716"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/2.11M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1c6cc28a35204d829ed7ab7a69722c32"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/548 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9df7665447d14629b2823b4d25751933"}},"metadata":{}}]},{"cell_type":"code","source":"# from transformers import AutoProcessor, Blip2ForConditionalGeneration\n\n# processor = AutoProcessor.from_pretrained(\"shorok31/BLIP_LORA\")\n# model = Blip2ForConditionalGeneration.from_pretrained(\"shorok31/BLIP_LORA\", device_map=\"auto\", load_in_8bit=True)","metadata":{"execution":{"iopub.status.busy":"2024-04-12T23:54:39.901343Z","iopub.execute_input":"2024-04-12T23:54:39.901757Z","iopub.status.idle":"2024-04-12T23:54:52.597417Z","shell.execute_reply.started":"2024-04-12T23:54:39.901723Z","shell.execute_reply":"2024-04-12T23:54:52.596468Z"},"trusted":true},"execution_count":30,"outputs":[{"name":"stderr","text":"The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"27804d4747cc40788454de77c063bc55"}},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# !pip install accelerate","metadata":{"execution":{"iopub.status.busy":"2024-04-12T16:17:10.616563Z","iopub.execute_input":"2024-04-12T16:17:10.617181Z","iopub.status.idle":"2024-04-12T16:17:10.621094Z","shell.execute_reply.started":"2024-04-12T16:17:10.617153Z","shell.execute_reply":"2024-04-12T16:17:10.620184Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"from peft import LoraConfig, get_peft_model\n\n# Let's define the LoraConfig\nconfig = LoraConfig(\n    r=16,\n    lora_alpha=32,\n    lora_dropout=0.05,\n    bias=\"none\",\n    target_modules=[\"q_proj\", \"k_proj\"]\n)\n\nmodel = get_peft_model(model, config)\nmodel.print_trainable_parameters()","metadata":{"execution":{"iopub.status.busy":"2024-04-13T01:37:55.730693Z","iopub.execute_input":"2024-04-13T01:37:55.731075Z","iopub.status.idle":"2024-04-13T01:37:55.897855Z","shell.execute_reply.started":"2024-04-13T01:37:55.731042Z","shell.execute_reply":"2024-04-13T01:37:55.896933Z"},"trusted":true},"execution_count":42,"outputs":[{"name":"stdout","text":"trainable params: 5,242,880 || all params: 3,749,922,816 || trainable%: 0.13981301102065136\n","output_type":"stream"}]},{"cell_type":"code","source":"images_captions_df = images_captions_df.sample(frac=0.5, random_state=42) \n\nfrom sklearn.model_selection import train_test_split\n\ntrain_df,val_df =train_test_split(images_captions_df , test_size=0.2, shuffle=True, random_state=42)\n\ntrain_dataset = ImageCaptioningDataset(train_df, processor)\ntrain_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=4, collate_fn=collate_fn)\n\n","metadata":{"execution":{"iopub.status.busy":"2024-04-13T01:38:05.073631Z","iopub.execute_input":"2024-04-13T01:38:05.074001Z","iopub.status.idle":"2024-04-13T01:38:05.084542Z","shell.execute_reply.started":"2024-04-13T01:38:05.073971Z","shell.execute_reply":"2024-04-13T01:38:05.083335Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"code","source":"val_df.head()","metadata":{"execution":{"iopub.status.busy":"2024-04-13T01:38:09.870371Z","iopub.execute_input":"2024-04-13T01:38:09.870720Z","iopub.status.idle":"2024-04-13T01:38:09.880345Z","shell.execute_reply.started":"2024-04-13T01:38:09.870694Z","shell.execute_reply":"2024-04-13T01:38:09.879357Z"},"trusted":true},"execution_count":44,"outputs":[{"execution_count":44,"output_type":"execute_result","data":{"text/plain":"                                                   imgs  \\\n2318  /kaggle/input/chest-xrays-indiana-university/i...   \n1789  /kaggle/input/chest-xrays-indiana-university/i...   \n3018  /kaggle/input/chest-xrays-indiana-university/i...   \n1187  /kaggle/input/chest-xrays-indiana-university/i...   \n4231  /kaggle/input/chest-xrays-indiana-university/i...   \n\n                                               captions  \n2318  Cardiac silhouette and pulmonary vascularity a...  \n1789  There is a calcified granuloma in the left upp...  \n3018  The cardiac and mediastinal silhouettes are un...  \n1187  The heart, pulmonary XXXX and mediastinum are ...  \n4231  Heart size and mediastinal contour are normal....  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>imgs</th>\n      <th>captions</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2318</th>\n      <td>/kaggle/input/chest-xrays-indiana-university/i...</td>\n      <td>Cardiac silhouette and pulmonary vascularity a...</td>\n    </tr>\n    <tr>\n      <th>1789</th>\n      <td>/kaggle/input/chest-xrays-indiana-university/i...</td>\n      <td>There is a calcified granuloma in the left upp...</td>\n    </tr>\n    <tr>\n      <th>3018</th>\n      <td>/kaggle/input/chest-xrays-indiana-university/i...</td>\n      <td>The cardiac and mediastinal silhouettes are un...</td>\n    </tr>\n    <tr>\n      <th>1187</th>\n      <td>/kaggle/input/chest-xrays-indiana-university/i...</td>\n      <td>The heart, pulmonary XXXX and mediastinum are ...</td>\n    </tr>\n    <tr>\n      <th>4231</th>\n      <td>/kaggle/input/chest-xrays-indiana-university/i...</td>\n      <td>Heart size and mediastinal contour are normal....</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"val_dataset = ImageCaptioningDataset(val_df, processor)\nval_dataloader = DataLoader(val_dataset, shuffle=True, batch_size=4, collate_fn=collate_fn)","metadata":{"execution":{"iopub.status.busy":"2024-04-13T01:38:32.396040Z","iopub.execute_input":"2024-04-13T01:38:32.396642Z","iopub.status.idle":"2024-04-13T01:38:32.411943Z","shell.execute_reply.started":"2024-04-13T01:38:32.396612Z","shell.execute_reply":"2024-04-13T01:38:32.410984Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"code","source":"import torch\nimport numpy as np\nimport tqdm\n\noptimizer = torch.optim.Adam(model.parameters(), lr=5e-4)\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\nmodel.train()\n\nfor epoch in range(1,11):\n    epoch_loss = []\n    \n    for idx, batch in tqdm.tqdm(enumerate(train_dataloader)):\n        \n        input_ids = batch.pop(\"input_ids\").to(device)\n        pixel_values = batch.pop(\"pixel_values\").to(device, torch.float16)\n\n        outputs = model(input_ids=input_ids,\n                    pixel_values=pixel_values,\n                    labels=input_ids)\n    \n        loss = outputs.loss\n\n        epoch_loss.append( loss.item())\n\n        loss.backward()\n\n        optimizer.step()\n        optimizer.zero_grad()\n        if( idx %100 ==0):\n            print(f\"Batch: {idx} , Loss: {np.mean(epoch_loss)}\")\n    print(f\"Epoch: {epoch}, Loss:{np.mean(epoch_loss) }\")","metadata":{"execution":{"iopub.status.busy":"2024-04-13T01:38:33.243951Z","iopub.execute_input":"2024-04-13T01:38:33.244266Z","iopub.status.idle":"2024-04-13T01:57:00.175796Z","shell.execute_reply.started":"2024-04-13T01:38:33.244241Z","shell.execute_reply":"2024-04-13T01:57:00.174484Z"},"trusted":true},"execution_count":46,"outputs":[{"name":"stderr","text":"1it [00:01,  1.97s/it]","output_type":"stream"},{"name":"stdout","text":"Batch: 0 , Loss: 5.12890625\n","output_type":"stream"},{"name":"stderr","text":"101it [02:19,  1.36s/it]","output_type":"stream"},{"name":"stdout","text":"Batch: 100 , Loss: 2.2465965346534653\n","output_type":"stream"},{"name":"stderr","text":"201it [04:36,  1.38s/it]","output_type":"stream"},{"name":"stdout","text":"Batch: 200 , Loss: 1.8715407338308458\n","output_type":"stream"},{"name":"stderr","text":"301it [06:52,  1.40s/it]","output_type":"stream"},{"name":"stdout","text":"Batch: 300 , Loss: 1.6942937603820598\n","output_type":"stream"},{"name":"stderr","text":"324it [07:24,  1.37s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 1, Loss:1.6683952425733024\n","output_type":"stream"},{"name":"stderr","text":"1it [00:01,  1.36s/it]","output_type":"stream"},{"name":"stdout","text":"Batch: 0 , Loss: 0.91552734375\n","output_type":"stream"},{"name":"stderr","text":"101it [02:15,  1.37s/it]","output_type":"stream"},{"name":"stdout","text":"Batch: 100 , Loss: 1.2245271890470297\n","output_type":"stream"},{"name":"stderr","text":"201it [04:30,  1.34s/it]","output_type":"stream"},{"name":"stdout","text":"Batch: 200 , Loss: 1.2010771338619404\n","output_type":"stream"},{"name":"stderr","text":"301it [06:46,  1.36s/it]","output_type":"stream"},{"name":"stdout","text":"Batch: 300 , Loss: 1.1842296511627908\n","output_type":"stream"},{"name":"stderr","text":"324it [07:17,  1.35s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 2, Loss:1.177324459876543\n","output_type":"stream"},{"name":"stderr","text":"1it [00:01,  1.32s/it]","output_type":"stream"},{"name":"stdout","text":"Batch: 0 , Loss: 0.96142578125\n","output_type":"stream"},{"name":"stderr","text":"101it [02:16,  1.35s/it]","output_type":"stream"},{"name":"stdout","text":"Batch: 100 , Loss: 1.0707040918935644\n","output_type":"stream"},{"name":"stderr","text":"166it [03:44,  1.35s/it]\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[46], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m11\u001b[39m):\n\u001b[1;32m     12\u001b[0m     epoch_loss \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 14\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m idx, batch \u001b[38;5;129;01min\u001b[39;00m tqdm\u001b[38;5;241m.\u001b[39mtqdm(\u001b[38;5;28menumerate\u001b[39m(train_dataloader)):\n\u001b[1;32m     16\u001b[0m         input_ids \u001b[38;5;241m=\u001b[39m batch\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     17\u001b[0m         pixel_values \u001b[38;5;241m=\u001b[39m batch\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpixel_values\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device, torch\u001b[38;5;241m.\u001b[39mfloat16)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tqdm/std.py:1182\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1179\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1181\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1182\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1183\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m   1184\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1185\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:674\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    672\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    673\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 674\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    675\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    676\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n","Cell \u001b[0;32mIn[4], line 18\u001b[0m, in \u001b[0;36mImageCaptioningDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     15\u001b[0m image \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mopen(image_path)\u001b[38;5;241m.\u001b[39mconvert(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRGB\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     16\u001b[0m caption \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcaptions[idx]\n\u001b[0;32m---> 18\u001b[0m encoding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocessor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_length\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# remove batch dimension\u001b[39;00m\n\u001b[1;32m     20\u001b[0m encoding \u001b[38;5;241m=\u001b[39m {k: v\u001b[38;5;241m.\u001b[39msqueeze() \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m encoding\u001b[38;5;241m.\u001b[39mitems()}\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/blip_2/processing_blip_2.py:105\u001b[0m, in \u001b[0;36mBlip2Processor.__call__\u001b[0;34m(self, images, text, add_special_tokens, padding, truncation, max_length, stride, pad_to_multiple_of, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_token_type_ids, return_length, verbose, return_tensors, **kwargs)\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m text_encoding\n\u001b[1;32m    104\u001b[0m \u001b[38;5;66;03m# add pixel_values\u001b[39;00m\n\u001b[0;32m--> 105\u001b[0m encoding_image_processor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimage_processor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    107\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m text \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    108\u001b[0m     text_encoding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer(\n\u001b[1;32m    109\u001b[0m         text\u001b[38;5;241m=\u001b[39mtext,\n\u001b[1;32m    110\u001b[0m         add_special_tokens\u001b[38;5;241m=\u001b[39madd_special_tokens,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    124\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    125\u001b[0m     )\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/image_processing_utils.py:551\u001b[0m, in \u001b[0;36mBaseImageProcessor.__call__\u001b[0;34m(self, images, **kwargs)\u001b[0m\n\u001b[1;32m    549\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, images, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BatchFeature:\n\u001b[1;32m    550\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Preprocess an image or a batch of images.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpreprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/blip/image_processing_blip.py:271\u001b[0m, in \u001b[0;36mBlipImageProcessor.preprocess\u001b[0;34m(self, images, do_resize, size, resample, do_rescale, rescale_factor, do_normalize, image_mean, image_std, return_tensors, do_convert_rgb, data_format, input_data_format, **kwargs)\u001b[0m\n\u001b[1;32m    268\u001b[0m     input_data_format \u001b[38;5;241m=\u001b[39m infer_channel_dimension_format(images[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m    270\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m do_resize:\n\u001b[0;32m--> 271\u001b[0m     images \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    272\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresize(image\u001b[38;5;241m=\u001b[39mimage, size\u001b[38;5;241m=\u001b[39msize, resample\u001b[38;5;241m=\u001b[39mresample, input_data_format\u001b[38;5;241m=\u001b[39minput_data_format)\n\u001b[1;32m    273\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m image \u001b[38;5;129;01min\u001b[39;00m images\n\u001b[1;32m    274\u001b[0m     ]\n\u001b[1;32m    276\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m do_rescale:\n\u001b[1;32m    277\u001b[0m     images \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    278\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrescale(image\u001b[38;5;241m=\u001b[39mimage, scale\u001b[38;5;241m=\u001b[39mrescale_factor, input_data_format\u001b[38;5;241m=\u001b[39minput_data_format)\n\u001b[1;32m    279\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m image \u001b[38;5;129;01min\u001b[39;00m images\n\u001b[1;32m    280\u001b[0m     ]\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/blip/image_processing_blip.py:272\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    268\u001b[0m     input_data_format \u001b[38;5;241m=\u001b[39m infer_channel_dimension_format(images[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m    270\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m do_resize:\n\u001b[1;32m    271\u001b[0m     images \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m--> 272\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_data_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_data_format\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    273\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m image \u001b[38;5;129;01min\u001b[39;00m images\n\u001b[1;32m    274\u001b[0m     ]\n\u001b[1;32m    276\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m do_rescale:\n\u001b[1;32m    277\u001b[0m     images \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    278\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrescale(image\u001b[38;5;241m=\u001b[39mimage, scale\u001b[38;5;241m=\u001b[39mrescale_factor, input_data_format\u001b[38;5;241m=\u001b[39minput_data_format)\n\u001b[1;32m    279\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m image \u001b[38;5;129;01min\u001b[39;00m images\n\u001b[1;32m    280\u001b[0m     ]\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/blip/image_processing_blip.py:150\u001b[0m, in \u001b[0;36mBlipImageProcessor.resize\u001b[0;34m(self, image, size, resample, data_format, input_data_format, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe `size` dictionary must contain the keys `height` and `width`. Got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msize\u001b[38;5;241m.\u001b[39mkeys()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    149\u001b[0m output_size \u001b[38;5;241m=\u001b[39m (size[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mheight\u001b[39m\u001b[38;5;124m\"\u001b[39m], size[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwidth\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m--> 150\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    152\u001b[0m \u001b[43m    \u001b[49m\u001b[43msize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    153\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresample\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    154\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    155\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_data_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_data_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    156\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    157\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/image_transforms.py:330\u001b[0m, in \u001b[0;36mresize\u001b[0;34m(image, size, resample, reducing_gap, data_format, return_numpy, input_data_format)\u001b[0m\n\u001b[1;32m    328\u001b[0m height, width \u001b[38;5;241m=\u001b[39m size\n\u001b[1;32m    329\u001b[0m \u001b[38;5;66;03m# PIL images are in the format (width, height)\u001b[39;00m\n\u001b[0;32m--> 330\u001b[0m resized_image \u001b[38;5;241m=\u001b[39m \u001b[43mimage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwidth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheight\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreducing_gap\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreducing_gap\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    332\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_numpy:\n\u001b[1;32m    333\u001b[0m     resized_image \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(resized_image)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/PIL/Image.py:2193\u001b[0m, in \u001b[0;36mImage.resize\u001b[0;34m(self, size, resample, box, reducing_gap)\u001b[0m\n\u001b[1;32m   2185\u001b[0m             \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mreduce(\u001b[38;5;28mself\u001b[39m, factor, box\u001b[38;5;241m=\u001b[39mreduce_box)\n\u001b[1;32m   2186\u001b[0m         box \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   2187\u001b[0m             (box[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m-\u001b[39m reduce_box[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;241m/\u001b[39m factor_x,\n\u001b[1;32m   2188\u001b[0m             (box[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m-\u001b[39m reduce_box[\u001b[38;5;241m1\u001b[39m]) \u001b[38;5;241m/\u001b[39m factor_y,\n\u001b[1;32m   2189\u001b[0m             (box[\u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m-\u001b[39m reduce_box[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;241m/\u001b[39m factor_x,\n\u001b[1;32m   2190\u001b[0m             (box[\u001b[38;5;241m3\u001b[39m] \u001b[38;5;241m-\u001b[39m reduce_box[\u001b[38;5;241m1\u001b[39m]) \u001b[38;5;241m/\u001b[39m factor_y,\n\u001b[1;32m   2191\u001b[0m         )\n\u001b[0;32m-> 2193\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_new(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbox\u001b[49m\u001b[43m)\u001b[49m)\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"code","source":"# prepare image for the model\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\ndef generate_caption(img_path):\n    image = Image.open(img_path).convert(\"RGB\")\n    inputs = processor(images=image, return_tensors=\"pt\").to(device, torch.float16)\n    pixel_values = inputs.pixel_values\n\n    generated_ids = model.generate(pixel_values=pixel_values, max_length=1024)\n    generated_caption = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n    return generated_caption\n\nprint(generate_caption(images_captions_df['imgs'][0]))","metadata":{"execution":{"iopub.status.busy":"2024-04-13T01:57:07.240355Z","iopub.execute_input":"2024-04-13T01:57:07.241029Z","iopub.status.idle":"2024-04-13T01:57:07.397093Z","shell.execute_reply.started":"2024-04-13T01:57:07.240995Z","shell.execute_reply":"2024-04-13T01:57:07.395895Z"},"trusted":true},"execution_count":47,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/core/indexes/base.py:3802\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3801\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3802\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3803\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n","File \u001b[0;32mindex.pyx:153\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n","File \u001b[0;32mindex.pyx:182\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n","File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:2606\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[0;34m()\u001b[0m\n","File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:2630\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[0;34m()\u001b[0m\n","\u001b[0;31mKeyError\u001b[0m: 0","\nThe above exception was the direct cause of the following exception:\n","\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)","Cell \u001b[0;32mIn[47], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m     generated_caption \u001b[38;5;241m=\u001b[39m processor\u001b[38;5;241m.\u001b[39mbatch_decode(generated_ids, skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m generated_caption\n\u001b[0;32m---> 13\u001b[0m \u001b[38;5;28mprint\u001b[39m(generate_caption(\u001b[43mimages_captions_df\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mimgs\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m))\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/core/series.py:1111\u001b[0m, in \u001b[0;36mSeries.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1108\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[key]\n\u001b[1;32m   1110\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m key_is_scalar:\n\u001b[0;32m-> 1111\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1113\u001b[0m \u001b[38;5;66;03m# Convert generator to list before going through hashable part\u001b[39;00m\n\u001b[1;32m   1114\u001b[0m \u001b[38;5;66;03m# (We will iterate through the generator there to check for slices)\u001b[39;00m\n\u001b[1;32m   1115\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/core/series.py:1227\u001b[0m, in \u001b[0;36mSeries._get_value\u001b[0;34m(self, label, takeable)\u001b[0m\n\u001b[1;32m   1224\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[label]\n\u001b[1;32m   1226\u001b[0m \u001b[38;5;66;03m# Similar to Index.get_value, but we do not fall back to positional\u001b[39;00m\n\u001b[0;32m-> 1227\u001b[0m loc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1229\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(loc):\n\u001b[1;32m   1230\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[loc]\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/core/indexes/base.py:3809\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   3805\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m   3806\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[1;32m   3807\u001b[0m     ):\n\u001b[1;32m   3808\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[0;32m-> 3809\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3810\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3811\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3812\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3813\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3814\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n","\u001b[0;31mKeyError\u001b[0m: 0"],"ename":"KeyError","evalue":"0","output_type":"error"}]},{"cell_type":"code","source":"import tqdm \n\n\npredicted_captions = [] \nfor i in tqdm.tqdm( val_df['imgs'].values[0:6]):\n    predicted_captions.append(generate_caption(i))\nprint(len(predicted_captions))","metadata":{"execution":{"iopub.status.busy":"2024-04-13T01:57:12.134947Z","iopub.execute_input":"2024-04-13T01:57:12.135287Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":"  0%|          | 0/6 [00:00<?, ?it/s]The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"}]},{"cell_type":"code","source":"import nltk\nfrom nltk.translate.bleu_score import corpus_bleu, SmoothingFunction\n\n# Assuming you have a list of predicted captions and a list of ground truth captions\ngenerated_captions = predicted_captions\nground_truth_captions = val_df['captions'].values[0:6]\n# Convert the caption lists into the format expected by nltk\nground_truth_captions = [[caption.split() for caption in captions] for captions in ground_truth_captions]\ngenerated_captions = [caption.split() for caption in generated_captions]\n\n\n# Define the smoothing function to use\n# smoothie = SmoothingFunction().method4\n\n# Compute the BLEU score with smoothing\nweights = (0.25, 0.25, 0.25, 0.25)  # equal weights for 1-4 gram BLEU scores\nscore = corpus_bleu(ground_truth_captions, predicted_captions,weights =weights)\nprint(f'The BELU Score Is: {score}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2024-04-12T23:43:59.047999Z","iopub.execute_input":"2024-04-12T23:43:59.048845Z","iopub.status.idle":"2024-04-12T23:44:10.399814Z","shell.execute_reply.started":"2024-04-12T23:43:59.048798Z","shell.execute_reply":"2024-04-12T23:44:10.398725Z"},"trusted":true},"execution_count":27,"outputs":[{"name":"stderr","text":"The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a89e73dd4ced4d8b8670346f10b4a5bc"}},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from huggingface_hub import notebook_login\n\nnotebook_login()","metadata":{"execution":{"iopub.status.busy":"2024-04-13T01:32:46.594774Z","iopub.execute_input":"2024-04-13T01:32:46.595116Z","iopub.status.idle":"2024-04-13T01:32:46.618245Z","shell.execute_reply.started":"2024-04-13T01:32:46.595091Z","shell.execute_reply":"2024-04-13T01:32:46.617379Z"},"trusted":true},"execution_count":38,"outputs":[{"output_type":"display_data","data":{"text/plain":"VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6ddfed46db16477fbf47f8e02d8a634c"}},"metadata":{}}]},{"cell_type":"code","source":"model.push_to_hub(\"shorok31/BLIP_LORA_2\")\nprocessor.push_to_hub(\"shorok31/BLIP_LORA_2\")","metadata":{"execution":{"iopub.status.busy":"2024-04-13T01:33:54.977469Z","iopub.execute_input":"2024-04-13T01:33:54.977850Z","iopub.status.idle":"2024-04-13T01:34:00.612132Z","shell.execute_reply.started":"2024-04-13T01:33:54.977819Z","shell.execute_reply":"2024-04-13T01:34:00.611216Z"},"trusted":true},"execution_count":39,"outputs":[{"output_type":"display_data","data":{"text/plain":"adapter_model.safetensors:   0%|          | 0.00/21.0M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"098b8eb6ddde4612aa72710ed4ec2c91"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/5.18k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8bcdd163c8c843658e45a5f889f4f339"}},"metadata":{}},{"execution_count":39,"output_type":"execute_result","data":{"text/plain":"CommitInfo(commit_url='https://huggingface.co/shorok31/BLIP_LORA_2/commit/d3210f2628c56e64b77585090c4b5adba98a3532', commit_message='Upload processor', commit_description='', oid='d3210f2628c56e64b77585090c4b5adba98a3532', pr_url=None, pr_revision=None, pr_num=None)"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.save(model.state_dict(), 'model.pth')","metadata":{"execution":{"iopub.status.busy":"2024-03-16T01:28:14.961253Z","iopub.execute_input":"2024-03-16T01:28:14.962347Z","iopub.status.idle":"2024-03-16T01:28:27.927603Z","shell.execute_reply.started":"2024-03-16T01:28:14.962304Z","shell.execute_reply":"2024-03-16T01:28:27.926491Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.save()","metadata":{"execution":{"iopub.status.busy":"2024-03-16T01:26:44.839439Z","iopub.execute_input":"2024-03-16T01:26:44.840103Z","iopub.status.idle":"2024-03-16T01:26:45.377668Z","shell.execute_reply.started":"2024-03-16T01:26:44.840071Z","shell.execute_reply":"2024-03-16T01:26:45.376487Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}